{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5d581-d388-45db-9994-511bf3306acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an FNO for the Darcy flow Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193b0e-377b-4c88-a036-b97719b0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0da88b-b81a-4d3d-b447-14c63d3f8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from FNO2D import *\n",
    "from utilities3 import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebda35b7-8310-40a6-8572-4a024c5e2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "reader = MatReader('df_train_128.mat')\n",
    "coeff = reader.read_field('coeff')\n",
    "sol = reader.read_field('sol')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67747f5b-01d2-4093-adb0-c8caae79005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/ validation : 80 / 20\n",
    "x_train, x_val, y_train, y_val = train_test_split(coeff, sol, random_state=0, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470ef436-fb5c-4684-80c4-e271b4b1a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters # \n",
    "\n",
    "ntrain = 800\n",
    "nval = 200\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 100\n",
    "step_size = 20\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "\n",
    "s = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2453b77-598e-402d-b1b5-6b649fd6eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalisation\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_val = x_normalizer.encode(x_val)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_val = x_val.reshape(nval,s,s,1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_val, y_val), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2070dab0-f2b4-45f4-bb4b-6b57b1ef7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 Time: 362.11250189878047 Train L: 0.09227138303220273 Val L: 0.047715665102005006\n",
      "Ep: 1 Time: 721.6687843063846 Train L: 0.04679346866905689 Val L: 0.04577301174402237\n",
      "Ep: 2 Time: 1083.8692088825628 Train L: 0.045174507200717924 Val L: 0.0432310476899147\n",
      "Ep: 3 Time: 1443.8173845093697 Train L: 0.04027597136795521 Val L: 0.03754954010248184\n",
      "Ep: 4 Time: 1803.7922482276335 Train L: 0.034624925814568996 Val L: 0.02924569383263588\n",
      "Ep: 5 Time: 2165.0937876589596 Train L: 0.03086181778460741 Val L: 0.02881988450884819\n",
      "Ep: 6 Time: 2525.8056990392506 Train L: 0.02948594782501459 Val L: 0.030938892811536788\n",
      "Ep: 7 Time: 2887.559965084307 Train L: 0.031622606739401815 Val L: 0.02823963239789009\n",
      "Ep: 8 Time: 3248.08435650263 Train L: 0.028625593818724156 Val L: 0.028022406399250032\n",
      "Ep: 9 Time: 3608.4364790041 Train L: 0.02824628561735153 Val L: 0.027430027723312378\n",
      "Ep: 10 Time: 3970.9337970241904 Train L: 0.029693724848330022 Val L: 0.027395018190145493\n",
      "Ep: 11 Time: 4330.832236493938 Train L: 0.02769181124866009 Val L: 0.02744767501950264\n",
      "Ep: 12 Time: 4693.203557417728 Train L: 0.027668013274669646 Val L: 0.02572767809033394\n",
      "Ep: 13 Time: 5052.838258997537 Train L: 0.028535402119159697 Val L: 0.026657409369945526\n",
      "Ep: 14 Time: 5412.985335486941 Train L: 0.028182119652628897 Val L: 0.030183530747890472\n",
      "Ep: 15 Time: 5776.055310749449 Train L: 0.026882609613239766 Val L: 0.026187981963157653\n",
      "Ep: 16 Time: 6136.482267165557 Train L: 0.02720287423580885 Val L: 0.02571404054760933\n",
      "Ep: 17 Time: 6498.890711371787 Train L: 0.027100637443363666 Val L: 0.025831414461135863\n",
      "Ep: 18 Time: 6860.063281109557 Train L: 0.026067050881683828 Val L: 0.02678182065486908\n",
      "Ep: 19 Time: 7220.648652140982 Train L: 0.026197508461773395 Val L: 0.025926231741905212\n",
      "Ep: 20 Time: 7584.804838154465 Train L: 0.02483705513179302 Val L: 0.027569646686315535\n",
      "Ep: 21 Time: 7946.601317632943 Train L: 0.024654584899544717 Val L: 0.025339258313179017\n",
      "Ep: 23 Time: 8669.777287090197 Train L: 0.023960907571017744 Val L: 0.025548652559518815\n",
      "Ep: 24 Time: 9031.725323662162 Train L: 0.023891667276620863 Val L: 0.025892697870731354\n",
      "Ep: 25 Time: 9393.776009146124 Train L: 0.02381502289324999 Val L: 0.025795066058635713\n",
      "Ep: 26 Time: 9753.34939163737 Train L: 0.02329264398664236 Val L: 0.02591838449239731\n",
      "Ep: 27 Time: 10114.322954624891 Train L: 0.023535657152533532 Val L: 0.026636047065258028\n",
      "Ep: 28 Time: 10473.328852768056 Train L: 0.023221992291510105 Val L: 0.026670973151922225\n",
      "Ep: 29 Time: 10832.750496826135 Train L: 0.023348510339856146 Val L: 0.026271245330572127\n",
      "Ep: 30 Time: 11194.010929942131 Train L: 0.02230854470282793 Val L: 0.026637768745422362\n",
      "Ep: 31 Time: 11554.144501494244 Train L: 0.022810252271592616 Val L: 0.026216548830270768\n",
      "Ep: 32 Time: 11915.329292458482 Train L: 0.022187519688159227 Val L: 0.0264714577794075\n",
      "Ep: 33 Time: 12274.682761566713 Train L: 0.02153286598622799 Val L: 0.026647879779338836\n",
      "Ep: 34 Time: 12634.270549953915 Train L: 0.021184464134275913 Val L: 0.027036412954330444\n",
      "Ep: 35 Time: 12996.853857154958 Train L: 0.021376360133290292 Val L: 0.026980365514755248\n",
      "Ep: 36 Time: 13356.790087921545 Train L: 0.020710508860647677 Val L: 0.027192135602235795\n",
      "Ep: 37 Time: 13718.493159714155 Train L: 0.020435374043881894 Val L: 0.02667196974158287\n",
      "Ep: 38 Time: 14078.199860022403 Train L: 0.019409350231289865 Val L: 0.026984636783599854\n",
      "Ep: 39 Time: 14438.488104883581 Train L: 0.019465939681977032 Val L: 0.02758130371570587\n",
      "Ep: 40 Time: 14800.647865438834 Train L: 0.018371684476733208 Val L: 0.026708448082208635\n",
      "Ep: 41 Time: 15160.225731668063 Train L: 0.018155743945389987 Val L: 0.026992403268814087\n",
      "Ep: 42 Time: 15522.03032380063 Train L: 0.018133160714060067 Val L: 0.02686751797795296\n",
      "Ep: 43 Time: 15882.10618883837 Train L: 0.017418760284781457 Val L: 0.02701765850186348\n",
      "Ep: 44 Time: 16241.847318791784 Train L: 0.017293978706002234 Val L: 0.027165002524852752\n",
      "Ep: 45 Time: 16603.146833840758 Train L: 0.017002520002424717 Val L: 0.02745110034942627\n",
      "Ep: 46 Time: 16964.11213613022 Train L: 0.016596891395747663 Val L: 0.027068073600530623\n",
      "Ep: 47 Time: 17325.52475693077 Train L: 0.01655584456399083 Val L: 0.02807481124997139\n",
      "Ep: 48 Time: 17685.82876739092 Train L: 0.016426888890564442 Val L: 0.0282682666182518\n",
      "Ep: 49 Time: 18046.063468503766 Train L: 0.01624172503128648 Val L: 0.027178270518779756\n",
      "Ep: 50 Time: 18408.530822888017 Train L: 0.01590082636103034 Val L: 0.027377553582191468\n",
      "Ep: 51 Time: 18768.75631798897 Train L: 0.015225390382111072 Val L: 0.02730374112725258\n",
      "Ep: 52 Time: 19129.50121311657 Train L: 0.015071078166365624 Val L: 0.027254232466220857\n",
      "Ep: 53 Time: 19490.01208634209 Train L: 0.014770466256886721 Val L: 0.027409071922302245\n",
      "Ep: 54 Time: 19850.595633964986 Train L: 0.014584451522678137 Val L: 0.027539032250642775\n",
      "Ep: 55 Time: 20212.547316244803 Train L: 0.014077463839203119 Val L: 0.027457717061042785\n",
      "Ep: 56 Time: 20573.597740332596 Train L: 0.013820922337472438 Val L: 0.02767465829849243\n",
      "Ep: 57 Time: 20935.850969618186 Train L: 0.013564079962670804 Val L: 0.027802252024412156\n",
      "Ep: 58 Time: 21295.594868280925 Train L: 0.013550037387758493 Val L: 0.027621121108531953\n",
      "Ep: 59 Time: 21656.520944873802 Train L: 0.013597672153264285 Val L: 0.028015848696231842\n",
      "Ep: 60 Time: 22019.309393215925 Train L: 0.01278205830603838 Val L: 0.027649400234222413\n",
      "Ep: 61 Time: 22380.14555027336 Train L: 0.01238690024241805 Val L: 0.027813431918621064\n",
      "Ep: 62 Time: 22742.301958138123 Train L: 0.012303890120238066 Val L: 0.02771866515278816\n",
      "Ep: 63 Time: 23103.091669202782 Train L: 0.012218200787901878 Val L: 0.027744779139757158\n",
      "Ep: 64 Time: 23463.60222706478 Train L: 0.012018421329557896 Val L: 0.02775502473115921\n",
      "Ep: 65 Time: 23825.808106734417 Train L: 0.011834191456437111 Val L: 0.027867950946092606\n",
      "Ep: 66 Time: 24187.6566409776 Train L: 0.011750435717403888 Val L: 0.028089311867952348\n",
      "Ep: 67 Time: 24549.545255422592 Train L: 0.01188678601756692 Val L: 0.027860485762357712\n",
      "Ep: 68 Time: 24909.608528733253 Train L: 0.011542070172727108 Val L: 0.027845934629440308\n",
      "Ep: 69 Time: 25270.143879084848 Train L: 0.01147310140542686 Val L: 0.027931007146835326\n",
      "Ep: 70 Time: 25632.73170770891 Train L: 0.011189214689657091 Val L: 0.02784347414970398\n",
      "Ep: 71 Time: 25992.58930550888 Train L: 0.011162948235869408 Val L: 0.027891584783792497\n",
      "Ep: 72 Time: 26354.30608072318 Train L: 0.01098704593256116 Val L: 0.02789932906627655\n",
      "Ep: 73 Time: 26715.288032615557 Train L: 0.010762121817097068 Val L: 0.02796387016773224\n",
      "Ep: 74 Time: 27076.907593033276 Train L: 0.01065018743276596 Val L: 0.028129310309886933\n",
      "Ep: 75 Time: 27439.592455687 Train L: 0.010555898603051901 Val L: 0.027933547198772432\n",
      "Ep: 76 Time: 27801.86740887817 Train L: 0.010441704960539937 Val L: 0.028028802871704103\n",
      "Ep: 77 Time: 28164.05291463714 Train L: 0.01032728496938944 Val L: 0.027991205751895905\n",
      "Ep: 78 Time: 28524.38956259098 Train L: 0.010202222447842359 Val L: 0.028018274903297426\n",
      "Ep: 79 Time: 28885.17411487922 Train L: 0.010149026392027735 Val L: 0.02798171877861023\n",
      "Ep: 80 Time: 29248.70603184309 Train L: 0.009901798777282238 Val L: 0.028040835559368135\n",
      "Ep: 81 Time: 29609.613985814154 Train L: 0.009815850174054503 Val L: 0.02805253565311432\n",
      "Ep: 82 Time: 29972.474368190393 Train L: 0.009776176931336523 Val L: 0.028068465888500215\n",
      "Ep: 83 Time: 30333.088656811044 Train L: 0.009599485276266933 Val L: 0.028077353537082673\n",
      "Ep: 84 Time: 30693.82849329617 Train L: 0.009553816998377443 Val L: 0.028079110980033874\n",
      "Ep: 85 Time: 31056.533847497776 Train L: 0.009523340817540884 Val L: 0.028068863600492478\n",
      "Ep: 86 Time: 31427.049688478 Train L: 0.009524257574230432 Val L: 0.0282475845515728\n",
      "Ep: 87 Time: 31789.122586072423 Train L: 0.009474065266549586 Val L: 0.028128040581941606\n",
      "Ep: 88 Time: 32148.74958022684 Train L: 0.009366214107722044 Val L: 0.028207494616508483\n",
      "Ep: 89 Time: 32508.6075445842 Train L: 0.009352768324315548 Val L: 0.02816026270389557\n",
      "Ep: 90 Time: 32870.42279228289 Train L: 0.009228784330189229 Val L: 0.028140552639961243\n",
      "Ep: 91 Time: 33229.64046078082 Train L: 0.009184828558936715 Val L: 0.02816420331597328\n",
      "Ep: 92 Time: 33591.126362278126 Train L: 0.009138026256114245 Val L: 0.028175709247589113\n",
      "Ep: 93 Time: 33950.145560016856 Train L: 0.009075835179537535 Val L: 0.028258473426103593\n",
      "Ep: 94 Time: 34309.655434506014 Train L: 0.009004582166671753 Val L: 0.02820619285106659\n",
      "Ep: 95 Time: 34670.67673606519 Train L: 0.008960108431056142 Val L: 0.028205036520957946\n",
      "Ep: 96 Time: 35030.81838254444 Train L: 0.008928644191473722 Val L: 0.02825536161661148\n",
      "Ep: 97 Time: 35392.34080547653 Train L: 0.00883063705638051 Val L: 0.028193490207195283\n",
      "Ep: 98 Time: 35751.19136337098 Train L: 0.008799473270773888 Val L: 0.02820902556180954\n",
      "Ep: 99 Time: 36111.71885322314 Train L: 0.008725023306906223 Val L: 0.028256727159023286\n"
     ]
    }
   ],
   "source": [
    "# training the FNO\n",
    "\n",
    "model = FNO2d(modes, modes, width) #.cuda()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "t1 = default_timer()\n",
    "\n",
    "# y_normalizer.cuda()\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    train_l2 = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        # x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x).reshape(batch_size, s, s)\n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            # x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            val_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    val_l2 /= nval\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(\"Ep:\",ep,\"Time:\", t2-t1, \"Train L:\", train_l2, \"Val L:\", val_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d6b27c-3a05-4acc-8193-6fa57e9376b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data to test the model\n",
    "\n",
    "reader = MatReader('df_test_256.mat')\n",
    "x_test_256 = reader.read_field('coeff')\n",
    "x_normalizer_256 = UnitGaussianNormalizer(x_test_256)\n",
    "y_test_256 = reader.read_field('sol')\n",
    "y_normalizer_256 = UnitGaussianNormalizer(y_test_256)\n",
    "x_test_256 = x_normalizer_256.encode(x_test_256)\n",
    "x_test_256 = x_test_256.reshape(200,256,256,1)\n",
    "\n",
    "\n",
    "\n",
    "reader.load_file('df_test_128.mat')\n",
    "x_test_128 = reader.read_field('coeff')\n",
    "x_normalizer_128 = UnitGaussianNormalizer(x_test_128)\n",
    "y_test_128 = reader.read_field('sol')\n",
    "y_normalizer_128 = UnitGaussianNormalizer(y_test_128)\n",
    "x_test_128 = x_normalizer_128.encode(x_test_128)\n",
    "x_test_128 = x_test_128.reshape(200,128,128,1)\n",
    "\n",
    "\n",
    "\n",
    "reader.load_file('df_test_64.mat')\n",
    "x_test_64 = reader.read_field('coeff')\n",
    "x_normalizer_64 = UnitGaussianNormalizer(x_test_64)\n",
    "y_test_64 = reader.read_field('sol')\n",
    "y_normalizer_64 = UnitGaussianNormalizer(y_test_64)\n",
    "x_test_64 = x_normalizer_64.encode(x_test_64)\n",
    "x_test_64 = x_test_64.reshape(200,64,64,1)\n",
    "\n",
    "test_loader_256 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_256, y_test_256), batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader_128 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_128, y_test_128), batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader_64 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_64, y_test_64), batch_size=1, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4816a1d2-d6e1-4400-99f9-5f8f042b7867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 256 Time taken: 126.03673484455794 Testing Loss: 0.034208891238085924\n"
     ]
    }
   ],
   "source": [
    "# Testing the FNO on 256 resolution\n",
    "\n",
    "mytestloss = LpLoss(size_average=False)\n",
    "t1 = default_timer()\n",
    "\n",
    "model.eval()\n",
    "test_l2 = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_256:\n",
    "                # x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "        out = model(x).reshape(1, 256, 256)\n",
    "        out = y_normalizer_256.decode(out)\n",
    "\n",
    "        test_l2 += mytestloss(out.view(1,-1), y.view(1,-1)).item()\n",
    "\n",
    "\n",
    "test_l2 /= 200\n",
    "\n",
    "t2 = default_timer()\n",
    "time_taken_256 = t2-t1\n",
    "loss_256 = test_l2\n",
    "print(\"resolution: 256\", \"Time taken:\", t2-t1, \"Testing Loss:\", test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059e2f3a-d989-4f59-95d3-9f6099a48750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 128 Time taken: 30.21675187535584 Testing Loss: 0.03279689413728192\n"
     ]
    }
   ],
   "source": [
    "# Testing the FNO on 128 resolution\n",
    "\n",
    "mytestloss = LpLoss(size_average=False)\n",
    "t1 = default_timer()\n",
    "\n",
    "model.eval()\n",
    "test_l2 = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_128:\n",
    "                # x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "        out = model(x).reshape(1, 128, 128)\n",
    "        out = y_normalizer_128.decode(out)\n",
    "\n",
    "        test_l2 += mytestloss(out.view(1,-1), y.view(1,-1)).item()\n",
    "\n",
    "\n",
    "test_l2 /= 200\n",
    "\n",
    "t2 = default_timer()\n",
    "time_taken_128 = t2-t1\n",
    "loss_128 = test_l2\n",
    "print(\"resolution: 128\", \"Time taken:\", t2-t1, \"Testing Loss:\", test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2334941f-ccc0-479a-aa1b-e9784e6eaf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 64 Time taken: 10.518008392304182 Testing Loss: 0.03542569748591631\n"
     ]
    }
   ],
   "source": [
    "# Testing the FNO on 64 resolution\n",
    "\n",
    "mytestloss = LpLoss(size_average=False)\n",
    "t1 = default_timer()\n",
    "\n",
    "model.eval()\n",
    "test_l2 = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_64:\n",
    "                # x, y = x.cuda(), y.cuda()\n",
    "            \n",
    "        out = model(x).reshape(1, 64, 64)\n",
    "        out = y_normalizer_64.decode(out)\n",
    "\n",
    "        test_l2 += mytestloss(out.view(1,-1), y.view(1,-1)).item()\n",
    "\n",
    "\n",
    "test_l2 /= 200\n",
    "\n",
    "t2 = default_timer()\n",
    "time_taken_64 = t2-t1\n",
    "loss_64 = test_l2\n",
    "print(\"resolution: 64\", \"Time taken:\", t2-t1, \"Testing Loss:\", test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5553272-3484-4af9-aec3-171e5d1c0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to compare to FDM\n",
    "resolution = [256, 128, 64]\n",
    "time_taken = [time_taken_256, time_taken_128, time_taken_64]\n",
    "testing_loss = [loss_256, loss_128, loss_64]\n",
    "scipy.io.savemat('df_FNO_results.mat', mdict={'res': resolution, 'time':time_taken, 'loss': testing_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a56984-230c-4e17-b688-2f10842a452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
